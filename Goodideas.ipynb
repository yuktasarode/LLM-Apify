{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install from PyPI\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnIaGfXD07tK",
        "outputId": "f79b09a0-88b9-4eb0-f706-d7be5e41626f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apify-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "494zoN5x2aqU",
        "outputId": "54b81411-9275-44b8-f55a-a66b50205fcf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apify-client\n",
            "  Downloading apify_client-1.7.0-py3-none-any.whl (71 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting apify-shared~=1.1.1 (from apify-client)\n",
            "  Downloading apify_shared-1.1.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: httpx>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from apify-client) (0.27.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->apify-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->apify-client) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->apify-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->apify-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.1->apify-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.1->apify-client) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.1->apify-client) (1.2.1)\n",
            "Installing collected packages: apify-shared, apify-client\n",
            "Successfully installed apify-client-1.7.0 apify-shared-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_inputs():\n",
        "    enterprise_name = input(\"Enterprise Name to search for: \")\n",
        "    keywords = input(\"Keyword(s) to search for: \")\n",
        "    insight_request = input(\"Type of insight to ask about the enterprise's data: \")\n",
        "    specific_websites = input(\"(Optional) Specific websites to scrape using available Apify actors or a separate website URL: \")\n",
        "    return enterprise_name, keywords, insight_request, specific_websites\n"
      ],
      "metadata": {
        "id": "ELhUfv1G0u85"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ak9ySVPCoFSv"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def expand_inputs(enterprise_name, keywords):\n",
        "    openai.api_key = 'your-api'\n",
        "\n",
        "    # response = openai.chat.completions.create(\n",
        "    # model=\"gpt-3.5-turbo\",\n",
        "    #  messages=[\n",
        "    #     {\"role\": \"system\", \"content\": \"You are an assistant skilled in generating common spelling errors and typos.\"},\n",
        "    #     {\"role\": \"user\", \"content\": f\"Generate common spelling errors and typos for the following:\\n\\nEnterprise Name: {enterprise_name}\\nKeyword(s): {keywords}\"}\n",
        "    # ],\n",
        "    # max_tokens=100)\n",
        "\n",
        "    # variations = response.choices[0].text.strip().split('\\n')\n",
        "    # enterprise_variations = [v for v in variations if \"Enterprise Name\" in v]\n",
        "    # keyword_variations = [v for v in variations if \"Keyword(s)\" in v]\n",
        "    enterprise_variations =[\"Targt\",\n",
        "    \"Targte\",\n",
        "    \"Taregt\"]\n",
        "    keyword_variations = [\"babby\",\n",
        "    \"babt\",\n",
        "    \"produts\",\n",
        "    \"prodacts\"]\n",
        "    enterprise_variations.append(enterprise_name)\n",
        "    keyword_variations.extend(keywords)\n",
        "\n",
        "    enterprise_variations = [str(word).lower() for word in enterprise_variations]\n",
        "    keyword_variations = [str(word).lower() for word in  keyword_variations]\n",
        "\n",
        "    return enterprise_variations, keyword_variations\n"
      ],
      "metadata": {
        "id": "7s3Nk2s60swE"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from apify_client import ApifyClient\n",
        "\n",
        "def scrape_data(enterprise_variations, keyword_variations, specific_websites=None):\n",
        "    client = ApifyClient('your-apify-api')\n",
        "\n",
        "    search_queries = [[ev, kv] for ev in enterprise_variations for kv in keyword_variations]  #generating combinations\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "\n",
        "    if specific_websites:\n",
        "        for website in specific_websites.split(','):\n",
        "            for query in search_queries:\n",
        "\n",
        "\n",
        "                run_input = {\n",
        "                    \"hashtags\": query,\n",
        "                    \"resultsLimit\": 3,\n",
        "                }\n",
        "                run = client.actor(\"apify/instagram-hashtag-scraper\").call(run_input=run_input)\n",
        "                for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
        "                  results.append(item)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "b2Z96EEq0ogp"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_rows(row, en, prd):\n",
        "\n",
        "    if isinstance(row, list):  # Check if row is a list\n",
        "        hashtags = [tag.lower() for tag in row]\n",
        "\n",
        "        found_list1 = any(word in hashtags for word in en) #ent\n",
        "        found_list2 = any(word in hashtags for word in prd) #key\n",
        "        # print(found_list1, found_list2)\n",
        "        if found_list1 and found_list2:\n",
        "          return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    else:\n",
        "        return False  # Return False if row is not a list\n"
      ],
      "metadata": {
        "id": "zzM2jsy2Sccj"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_data(results, insight_request):\n",
        "    openai.api_key = 'you-key'\n",
        "    context = f\"Data: {results}\\nInsight Request: {insight_request}\"\n",
        "\n",
        "    # response = openai.chat.completions.create(\n",
        "    #   model=\"gpt-3.5-turbo\",\n",
        "    #    messages=[\n",
        "    #     {\"role\": \"system\", \"content\": \"You are an assistant skilled in providing analytical insights of any data.\"},\n",
        "    #     {\"role\": \"user\", \"content\": context}\n",
        "    # ],\n",
        "    #   max_tokens=300\n",
        "    # )\n",
        "\n",
        "    # return response.choices[0].text.strip()\n",
        "\n",
        "    return \"Successfull\"\n"
      ],
      "metadata": {
        "id": "DvmqRoxw0j0Y"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFfT0Kdt0Zny",
        "outputId": "ae9a95df-f70f-4639-9934-9d3906ea49d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enterprise Name to search for: Target\n",
            "Keyword(s) to search for: Baby Products\n",
            "Type of insight to ask about the enterprise's data: For the majority of reviews, do people like or dislike Target's baby products? Please provide sources.\n",
            "(Optional) Specific websites to scrape using available Apify actors or a separate website URL: https://www/instagram.com/\n",
            "Inputs received\n",
            "Scraping data\n",
            "[{\"caption\":\"Sophi con este hermoso Ballon Romper de  @target \\ud83c\\udf38 y su mu\\u00f1equita de #leyadoll_com \\ud83d\\udc96\\n\\n#targetstyle #target #catandjackfortarget #babyclothing #babyclothes #baby #babygirl\",\"commentsCount\":0,\"hashtags\":[\"leyadoll_com\",\"targetstyle\",\"target\",\"catandjackfortarget\",\"babyclothing\",\"babyclothes\",\"baby\",\"babygirl\"],\"isSponsored\":false},{\"caption\":\"Sophi con este hermoso Ballon Romper de  @target \\ud83c\\udf38 y su mu\\u00f1equita de #leyadoll_com \\ud83d\\udc96\\n\\n#targetstyle #target #catandjackfortarget #babyclothing #babyclothes #baby #babygirl\",\"commentsCount\":0,\"hashtags\":[\"leyadoll_com\",\"targetstyle\",\"target\",\"catandjackfortarget\",\"babyclothing\",\"babyclothes\",\"baby\",\"babygirl\"],\"isSponsored\":false}]\n",
            "Generated Insights:\n",
            " Successfull\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    enterprise_name, keywords, insight_request, specific_websites = get_user_inputs()\n",
        "    keywords = keywords.split(',')\n",
        "    keywords = [word.strip() for keyword in keywords for word in keyword.split()]\n",
        "\n",
        "    enterprise_variations, keyword_variations = expand_inputs(enterprise_name, keywords)\n",
        "    print(\"Inputs received\")\n",
        "\n",
        "\n",
        "    print(\"Scraping data\")\n",
        "    #[targte, bby]\n",
        "\n",
        "    results = scrape_data(enterprise_variations, keyword_variations, specific_websites)\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "    desired_columns = ['caption', 'commentsCount','hashtags','isSponsored']\n",
        "    selected_df = df[desired_columns]\n",
        "    selected_df = selected_df.drop(df.index[0])\n",
        "    filtered_rows = selected_df[selected_df['hashtags'].apply(filter_rows, args=(enterprise_variations, keyword_variations))]\n",
        "    json_data = filtered_rows .to_json(orient='records')\n",
        "\n",
        "    print(json_data)\n",
        "\n",
        "\n",
        "    insights = analyze_data(json_data, insight_request)\n",
        "    print(\"Generated Insights:\\n\", insights)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZkAFqPqxpTNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}